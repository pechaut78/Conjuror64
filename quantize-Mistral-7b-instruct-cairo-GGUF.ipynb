{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "hub_name = \"pechaut/Mistral-C64Wizard-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8040e31104447989aea813d62c88770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pe_ch\\.cache\\huggingface\\hub\\models--pechaut--Mistral-C64Wizard-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13efea6b49f3447291ed3eb212c9c710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121090d2c8774c5e8093da436aa24b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063d2324a6074eb89461d2b65c6c3e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00015.safetensors:   0%|          | 0.00/900M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87febc2534ad4de784220eab7b706ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907cb0672610493cb4e0560a3dbfcaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e20464897b34c96b3480f21c6eeeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00015.safetensors:   0%|          | 0.00/956M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f0bff641144f5f9610433db966b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf8af594bb04977bf0c096a1bcd6204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49b047728824226951860a9e7ba93f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89385a4d5caa44bf9ebb2eea598bb0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00015.safetensors:   0%|          | 0.00/956M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a39c739d38480d9567a84fa6d42507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d15f065bf8c45a0921cc5e976a3b985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd62a28ba946ce89cdd359a3495c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e78a5f986b4e27b746ec62d002f9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00015.safetensors:   0%|          | 0.00/956M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a29fb1b6184a7fa2a829b746e4364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b199015498424784aa66f29fbd29be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a280810e043439f88bae35f035548dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00015.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be042118c1514634a10bc6a1d3df1138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebd958f3dec4f0a8417165d3fb0dff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hub_name = \"pechaut/Mistral-C64Wizard-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"cpu\",\n",
    "                                             offload_folder=\"offload\",\n",
    "                                             \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\projets\\Conjuror64\\llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/pechaut/Mistral-C64Wizard-instruct/resolve/main/tokenizer_config.json\n",
      "Error: 404\n",
      "https://huggingface.co/pechaut/Mistral-C64Wizard-instruct/resolve/main/tokenizer.model\n",
      "Error: 404\n",
      "https://huggingface.co/pechaut/Mistral-C64Wizard-instruct/resolve/main/tokenizer.json\n",
      "Error: 404\n",
      "https://huggingface.co/pechaut/Mistral-C64Wizard-instruct/resolve/main/special_tokens_map.json\n",
      "Error: 404\n",
      "https://huggingface.co/pechaut/Mistral-C64Wizard-instruct/resolve/main/added_tokens.json\n",
      "Error: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def dl_HF(model_name,filename,save_path):\n",
    " url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
    " print(url)\n",
    " r = requests.get(url, allow_redirects=True)\n",
    " if(r.status_code == 200):\n",
    "    open(f\"{save_path}{filename}\", 'wb').write(r.content)    \n",
    " else:\n",
    "    print(f\"Error: {r.status_code}\")\n",
    "\n",
    "files = [\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"tokenizer.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"added_tokens.json\",\n",
    "]\n",
    "\n",
    "save_path = \"./models/\"\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    dl_HF(hub_name,filename,save_path)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
      "  Using cached numpy-1.24.4.tar.gz (10.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "          backend = _build_backend()\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "          obj = import_module(mod_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\pe_ch\\anaconda3\\envs\\cairo-llm\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1304, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1325, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 929, in _load_unlocked\n",
      "        File \"<frozen importlib._bootstrap_external>\", line 994, in exec_module\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"C:\\Users\\pe_ch\\AppData\\Local\\Temp\\pip-build-env-ahng021v\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "          import setuptools.version\n",
      "        File \"C:\\Users\\pe_ch\\AppData\\Local\\Temp\\pip-build-env-ahng021v\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "          import pkg_resources\n",
      "        File \"C:\\Users\\pe_ch\\AppData\\Local\\Temp\\pip-build-env-ahng021v\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'make' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "!make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pec/projets/cairo-llm-refacto/3-quantize/llama.cpp/gguf-py\n",
      "Loading model file models/model-00001-of-00003.safetensors\n",
      "Loading model file models/model-00001-of-00003.safetensors\n",
      "Loading model file models/model-00002-of-00003.safetensors\n",
      "Loading model file models/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05, n_experts=None, n_experts_used=None, rope_scaling_type=None, f_rope_freq_base=1000000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('models/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('models/tokenizer.json')}\n",
      "Loading vocab file 'models/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing models/ggml-model-f16.gguf, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   0\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   7\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  10\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  10\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  10\n",
      "Wrote models/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized model to models/Mistral-7b-instruct-cairo.Q4_k.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 1917 (57e2a7a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'models/ggml-model-f16.gguf' to 'models/Mistral-7b-instruct-cairo.Q4_k.gguf' as Q4_K\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from models/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 735392 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   250.00 MiB ->    70.31 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  20/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  21/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  22/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  23/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  24/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  25/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  26/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  29/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  32/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  33/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  35/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  38/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  41/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  42/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  43/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  47/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  50/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  51/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  52/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  56/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  59/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  60/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  61/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  62/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  65/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  68/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  69/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  70/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  74/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  77/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  78/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  79/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  83/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  86/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  87/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  88/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  89/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  92/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  95/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  96/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  97/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 200/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 201/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 203/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB\n",
      "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 13813.02 MB\n",
      "llama_model_quantize_internal: quant size  =  4165.37 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 22640.03 ms\n",
      "main:    total time = 22640.03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['./quantize', 'models/ggml-model-f16.gguf', 'models/Mistral-7b-instruct-cairo.Q4_k.gguf', 'Q4_k'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = hub_name.split('/')\n",
    "model_name_pure = parts[1]\n",
    "quant_type = \"Q4_k\"\n",
    "quantized_model_name = f\"models/{model_name_pure}.{quant_type}.gguf\"\n",
    "print(f\"Saving quantized model to {quantized_model_name}\")\n",
    "import subprocess\n",
    "\n",
    "command = [\"./quantize\",'models/ggml-model-f16.gguf',quantized_model_name,quant_type]\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b5a34839dc42f88163d95a1189634c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mistral-7b-instruct-cairo.Q4_k.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "quant_name = hub_name.split('/')[-1] + \"-GGUF\"\n",
    "repo_id = \"StarkWizard/\" + quant_name\n",
    "base_path=\"./models\"\n",
    "\n",
    "local_file_paths = [\n",
    "                    \n",
    "                    base_path + \"/model.safetensors.index.json\",\n",
    "                    base_path + \"/tokenizer_config.json\",\n",
    "                    base_path + \"/tokenizer.json\",\n",
    "                    base_path + \"/tokenizer.model\",\n",
    "                    base_path + \"/\"+f\"{model_name_pure}.{quant_type}.gguf\",\n",
    "                    base_path + \"/config.json\",\n",
    "                    base_path + \"/generation_config.json\",\n",
    "                    base_path + \"/special_tokens_map.json\",\n",
    "                    ]\n",
    "for l in local_file_paths:\n",
    "    file_name = l.split(\"/\")[-1]\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=l,\n",
    "        path_in_repo=file_name,\n",
    "        repo_id=repo_id,\n",
    "        repo_type = \"model\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
